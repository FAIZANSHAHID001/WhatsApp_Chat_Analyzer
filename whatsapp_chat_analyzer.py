# -*- coding: utf-8 -*-
"""WhatsApp_Chat_Analyzer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BDXmhaws_n6ZmoxLmza-J47G5c0PJSiB
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re

from collections import Counter
from wordcloud import WordCloud
from googletrans import Translator
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import nltk
from nltk.tokenize import sent_tokenize
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer

# Load your WhatsApp chat data (replace with your actual data)
with open("WhatsApp Chat with School NibbasðŸ’¯.txt", "r", encoding="utf-8") as file:
    data = file.read()

# Define the regular expression pattern
pattern = r'(\d{1,2}/\d{1,2}/\d{4}),\s(\d{1,2}:\d{2}\s[APap][Mm])\s-\s(.*?):\s(.*)'

# Find all matches using the regular expression pattern
matches = re.findall(pattern, data)

# Create a list to store the extracted data
chat_data = []

# Iterate through the matches and extract data
for match in matches:
    date = match[0]
    time = match[1]
    username = match[2]
    message = str(match[3])  # Convert message to a string explicitly

    # Split the date into day, month, and year
    day, month, year = map(int, date.split('/'))

    # Append the data to the list
    chat_data.append([day, month, year, time, username, message])

# Create a pandas DataFrame from the extracted data
df = pd.DataFrame(chat_data, columns=["Day", "Month", "Year", "Time", "Username", "Message"])

# Basic statistics summary
print(df.describe())

# Count the number of messages per user
user_message_counts = df['Username'].value_counts()

# Visualize the number of messages per user
plt.figure(figsize=(10, 6))
sns.barplot(x=user_message_counts.index, y=user_message_counts.values)
plt.title("Number of Messages per User")
plt.xlabel("Usernames")
plt.ylabel("Message Count")
plt.xticks(rotation=45)
plt.show()

# Analyze the most active day
active_day = df['Day'].value_counts().idxmax()
print(f"The most active day is: {active_day}")

# Analyze the most active month
active_month = df['Month'].value_counts().idxmax()
print(f"The most active month is: {active_month}")

# Analyze the most active time
active_time = df['Time'].value_counts().idxmax()
print(f"The most active time is: {active_time}")

# Concatenate all messages into a single string
all_messages = " ".join(df['Message'])

# Generate a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_messages)

# Plot the word cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title("Word Cloud - Most Used Words")
plt.axis('off')
plt.show()

# Sample mixed-language paragraph (replace this with your actual 'mixed_language_paragraph')
mixed_language_paragraph = "This is a mixed-language paragraph with some text in different languages."

# Initialize the translator
translator = Translator()

# Split the mixed-language paragraph into sentences (assuming sentences end with periods)
sentences = sent_tokenize(mixed_language_paragraph)

# Translate each sentence and store the translations
english_sentences = []
for sentence in sentences:
    # Check if the sentence is empty before translating
    if sentence.strip():  # Skip empty sentences
        # Translate each sentence to English
        translation = translator.translate(sentence, src='auto', dest='en')
        english_sentences.append(translation.text)

# Combine the translated sentences into a single English paragraph
english_paragraph = " ".join(english_sentences)

# Print the translated paragraph
print(english_paragraph)

# Aggregate Messages into a Paragraph
messages_paragraph = " ".join(df['Message'])

# Download NLTK data (if not already downloaded)
nltk.download('vader_lexicon')

# Initialize the sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

# Analyze the messages
sentiment_scores = analyzer.polarity_scores(messages_paragraph)

# Print sentiment scores
print(sentiment_scores)

# Sample mixed-language paragraph (replace this with your actual 'english_translation')
mixed_language_paragraph = "This is a mixed-language paragraph with some text in different languages."

# Initialize the summarizer
summarizer = LsaSummarizer()

# Tokenize the mixed-language paragraph
parser = PlaintextParser.from_string(mixed_language_paragraph, Tokenizer("english"))

# Summarize the text (adjust the ratio as needed)
summary = summarizer(parser.document, 10)  # Adjust the ratio

# Generate and print the summary
for sentence in summary:
    print(sentence)

# Count the number of messages per user
user_message_counts = df['Username'].value_counts()

# Visualize the number of messages per user as a pie chart
plt.figure(figsize=(8, 8))
plt.pie(user_message_counts, labels=user_message_counts.index, autopct='%1.1f%%', startangle=140)
plt.title("Distribution of Messages by User")
plt.show()

# Analyze the distribution of messages by month
plt.figure(figsize=(10, 6))
sns.countplot(x='Month', data=df, palette='viridis')
plt.title("Distribution of Messages by Month")
plt.xlabel("Month")
plt.ylabel("Message Count")
plt.xticks(rotation=45)
plt.show()

# Analyze the distribution of messages by day of the week
df['Day'] = pd.to_datetime(df['Day'])
df['Day_of_Week'] = df['Day'].dt.day_name()

plt.figure(figsize=(10, 6))
sns.countplot(x='Day_of_Week', data=df, palette='Set3', order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])
plt.title("Distribution of Messages by Day of the Week")
plt.xlabel("Day of the Week")
plt.ylabel("Message Count")
plt.xticks(rotation=45)
plt.show()

# Load your WhatsApp chat data (replace with your actual data)
with open("WhatsApp Chat with School NibbasðŸ’¯.txt", "r", encoding="utf-8") as file:
    data = file.read()

# Define the regular expression pattern to extract messages
pattern = r'\d{1,2}/\d{1,2}/\d{4}, \d{1,2}:\d{2} [APap][Mm] - (.*?): (.*)'

# Find all matches using the regular expression pattern
matches = re.findall(pattern, data)

# Extract and tokenize the messages, while also extracting emojis
tokenized_messages = []
emojis_used = []

for match in matches:
    message = match[1]

    # Extract emojis from the message
    emojis = ''.join(c for c in message if c in emoji.UNICODE_EMOJI)

    if emojis:
        emojis_used.extend(emojis)

    # Tokenize the message (you can use a more advanced tokenizer if needed)
    tokens = re.findall(r'\w+', message)
    tokenized_messages.extend(tokens)

# Count the frequency of each emoji
emoji_counts = Counter(emojis_used)

# Create a DataFrame from the emoji counts
emoji_df = pd.DataFrame(emoji_counts.items(), columns=["Emoji", "Count"])

# Sort the DataFrame by emoji count in descending order
emoji_df = emoji_df.sort_values(by="Count", ascending=False)

# Print the top 10 most used emojis
top_emojis = emoji_df.head(10)
print(top_emojis)

# You can also visualize the top emojis if needed

print(sentence)

